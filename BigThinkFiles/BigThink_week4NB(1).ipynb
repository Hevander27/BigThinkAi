{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                 as np\n",
    "import pandas                as pd\n",
    "import matplotlib.pyplot     as plt\n",
    "import seaborn               as sns\n",
    "import glob\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "authors": [
     {
      "name": "Hevander Da Costa"
     }
    ],
    "title": "ABT, Impuation and Transformation"
   },
   "source": [
    "## Analytics Base Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reduce()](https://docs.python.org/3/library/functools.html#functools.reduce \"Documentation\") Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value\n",
    "\n",
    "[lambda](https://book.pythontips.com/en/latest/lambdas.html \"Documentation\") Lambdas are one line functions. They are also known as anonymous functions in some other languages. You might want to use lambdas when you don’t want to use a function twice in a program. They are just like normal functions and even behave like them.\n",
    "\n",
    "[pandas.merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html \"Documentation\") Merge DataFrame or named Series objects with a database-style join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(path_address):\n",
    "    \"\"\"\n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `path_address`: \n",
    "    \n",
    "    Outputs:\n",
    "    `df_merged`: \n",
    "    \"\"\"\n",
    "    \n",
    "    all_files = glob.glob(path_address + \"/*.csv\")\n",
    "    data_frames = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        data_frames.append(df)\n",
    "    \n",
    "    df_merged = reduce(lambda  left,right: pd.merge\\\n",
    "                (left,right,how='outer'), data_frames)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path string to a variable\n",
    "path = r'/BigThinkFiles'\n",
    "# Call function and set to a variable\n",
    "complaints_df = merge_csv(path)\n",
    "complaints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing column headers and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes within the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df.rename(columns = {'':''}, inplace = True)\n",
    "complaints_df.rename(columns = {'':''}, inplace = True)\n",
    "complaints_df['Year'] = complaints_df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes within the dataframe after edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edited dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report\n",
    "---\n",
    "Continous and Catagorical Quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "complaints_df_plot = sns.pairplot(complaints_df, hue=\"Borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a correlation matrix from the dataframe, hint: .corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.select_dtypes()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html \"Documentation\") Return a subset of the DataFrame’s columns based on the column dtypes.\n",
    "\n",
    "\n",
    "[pandas.isnull()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html \"Documentation\") Return a boolean same-sized object indicating if the values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continous_quality(frame):\n",
    "    \"\"\"\n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `frame`: \n",
    "    \n",
    "    Outputs:\n",
    "    `continous_df`: \n",
    "    \"\"\"\n",
    "    frame = frame.select_dtypes(include='float64')\n",
    "    percent_missing = frame.isnull().sum() * 100 / len(frame)\n",
    "    continous_df = pd.DataFrame({'Count': frame.count(),\n",
    "                                'percent_missing': percent_missing,\n",
    "                                'Card':  frame.nunique(),\n",
    "                                'Min': frame.min(),\n",
    "                                '1st Quartile': frame.quantile(.25),\n",
    "                                'Mean': frame.mean(),\n",
    "                                'Median':frame.median(),\n",
    "                                '3rd Quartile':frame.quantile(.75),\n",
    "                                'Max': frame.max(),\n",
    "                                'Std Dev':frame.std()})\n",
    "    continous_df.sort_values('percent_missing', inplace=True)\n",
    "    return continous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the continous_quality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features that have ~60% missing, hint: .drop(), axis=1\n",
    "# display the first five rows of the new altered dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Counter()](https://docs.python.org/3/library/collections.html#collections.Counter \"Documentation\") A collection where elements are stored as dictionary keys and their counts are stored as dictionary values.\n",
    "\n",
    "[most_common()](https://docs.python.org/3/library/collections.html#collections.Counter.most_common \"Documentation\") Return a list of the n most common elements and their counts from the most common to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data = Counter(complaints_df['UHF_42'])\n",
    "data.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp \"Documentation\") List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.\n",
    "\n",
    "[numpy.unique()](https://numpy.org/doc/stable/reference/generated/numpy.unique.html \"Documentation\") Returns the sorted unique elements of an array.\n",
    "\n",
    "[Series.map()](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html \"Documentation\") Map values of Series according to an input mapping or function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_quality(frame):\n",
    "    \"\"\"\n",
    "    Creates a dataframe containing data quality metrics\n",
    "    for each object and int64 feature\n",
    "    \n",
    "    Arguments:\n",
    "    `frame`: dataframe containing categorical variables\n",
    "    \n",
    "    Outputs:\n",
    "    `categorical_df`:Dataframe containing percent missing, cardinality,\n",
    "                    mode of each feature, count of the Mode, percent of\n",
    "                    the mode compared to all values, 2nd Mode, percent\n",
    "                    of the 2nd Mode.\n",
    "    \"\"\"\n",
    "\n",
    "    cat_frame = frame.select_dtypes(exclude='float64', include=['int64', 'object'])\n",
    "    percent_missing = cat_frame.isnull().sum() * 100 / len(cat_frame)\n",
    "    categorical_df = pd.DataFrame({ 'percent_missing': percent_missing,\n",
    "                                    'Card':  cat_frame.nunique(),\n",
    "                                    'Mode': [Counter(complaints_df[c]).most_common(len(np.unique(complaints_df[c])))\\\n",
    "                                    [0][0] for c in list(cat_frame.columns)],\n",
    "                                    'Mode Freq.': [Counter(complaints_df[c]).most_common(len(np.unique(complaints_df[c])))\\\n",
    "                                    [0][1] for c in list(cat_frame.columns)],\n",
    "                                    'Mode %': [Counter(complaints_df[c]).most_common(len(np.unique(complaints_df[c])))\\\n",
    "                                    [0][1] for c in list(cat_frame.columns)],\n",
    "                                    '2nd Mode': [Counter(complaints_df[c]).most_common(len(np.unique(complaints_df[c])))\\\n",
    "                                    [1][0] for c in list(cat_frame.columns)],\n",
    "                                    '2nd Mode %':  [Counter(complaints_df[c]).most_common(len(np.unique(complaints_df[c])))\\\n",
    "                                    [1][1] for c in list(cat_frame.columns)]\n",
    "                                 })\n",
    "    categorical_df['Mode %'] = categorical_df['Mode %'].div(len(cat_frame)) * 100\n",
    "    categorical_df['Mode %'] = categorical_df['Mode %'].map('{:,.2f}'.format)\n",
    "    categorical_df['2nd Mode %'] = categorical_df['2nd Mode %'].div(len(cat_frame)) * 100\n",
    "    categorical_df['2nd Mode %'] = categorical_df['2nd Mode %'].map('{:,.2f}'.format)\n",
    "    categorical_df.sort_values('percent_missing', inplace=True)\n",
    "    return categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the categorical_quality function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[numpy.logs()](https://numpy.org/doc/stable/reference/generated/numpy.log.html \"Documentation\") Natural logarithm, element-wise.\n",
    "\n",
    "[pandas.concat()](https://pandas.pydata.org/docs/reference/api/pandas.concat.html \"Documentation\") Concatenate pandas objects along a particular axis with optional set logic along the other axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logTransformation(df):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "\n",
    "    Arguments:\n",
    "    `df`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`:\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    logCounts = np.log(df_copy.select_dtypes(include=''))\n",
    "    labels = df_copy.select_dtypes(include=[''], exclude='')\n",
    "    result = pd.concat([labels, logCounts.reindex(labels.index)], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the logTransformation function as display the first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square Root Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtTransformation(df):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "\n",
    "    Arguments:\n",
    "    `df`:\n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    sqrtCounts = df_copy.select_dtypes(include='')**.5\n",
    "    labels = df_copy.select_dtypes(include=[''], exclude='')\n",
    "    result = pd.concat([labels, sqrtCounts.reindex(labels.index)], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the sqrtTransformation function as display the first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[zip()](https://www.programiz.com/python-programming/methods/built-in/zip \"Documentation\") The function takes iterables (can be zero or more), aggregates them in a tuple, and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "\n",
    "    Arguments:\n",
    "    `df`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    dtypes = list(zip(df_copy.dtypes.index, map(str, df_copy.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == '':\n",
    "            df_copy[column] -= df_copy[column].mean()\n",
    "            df_copy[column] /= df_copy[column].std()\n",
    "    result = df_copy\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the standardization function as display the first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clamp Transformation\n",
    "***\n",
    "Based on the outlier of your data you can use a bell\n",
    " curve to inform your percentile cut offs\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5c/PR_and_NCE.gif\" style=\"width:800px;height:400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the outlier of your data you can use a bell\n",
    "# curve to inform your percentile cut cut offs \n",
    "def clampTransformation(df):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "\n",
    "    Arguments:\n",
    "    `dataset`: \n",
    "    \n",
    "    Outputs:\n",
    "    `data`: \n",
    "    \"\"\"\n",
    "    \n",
    "    result = df.copy()\n",
    "    for c in list(df.columns):\n",
    "        if df[c].dtype == 'float64':\n",
    "            result[c] = result[c].apply(lambda x:  np.random.randint\\\n",
    "                        (result[c].quantile(0.34), result[c].quantile(0.68))\n",
    "                        if x > result[c].quantile(.94) \n",
    "                        or x < result[c].quantile(.03) else x )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the standardization function as display the first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQ-Plot and Histogram to check Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(x=complaints_df[''], dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot for ------\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vari_count = complaints_df['']\n",
    "D_close = vari_count.plot.hist()\n",
    "D_close.set_title('Histogram of ------')\n",
    "D_close.set_xlabel('count')\n",
    "plt.plot()\n",
    "plt.axvline(vari_count.mean(), color='y', linestyle='solid', linewidth=2)\n",
    "plt.axvline(vari_count.min(), color='r', linestyle='solid', linewidth=2)\n",
    "plt.axvline(vari_count.max(), color='r', linestyle='solid', linewidth=2)\n",
    "plt.axvline(vari_count.median(), color='g', linestyle='solid', linewidth=2)\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(vari_count.mean(), max_ylim*0.9, '{:.2f} (mean)'.format(vari_count.mean()))\n",
    "plt.text(vari_count.max(), max_ylim*1.2, '{:.2f}'.format(vari_count.max()))\n",
    "plt.text(vari_count.min(), max_ylim*1.2, '{:.2f}'.format(vari_count.min()))\n",
    "plt.text(vari_count.median(), max_ylim*0.75, '{:.2f} (median)'.format(vari_count.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box and whisker analysis\n",
    "***\n",
    "The box plot shape will show if a statistical data set is normally\n",
    "distributed or skewed. When the median is in the middle of the box, and the \n",
    "whiskers are about the same on both sides of the box, then the distribution is symmetric.\n",
    "***\n",
    "[Box and Whisker](https://www.simplypsychology.org/boxplots.html#:~:text=The%20box%20plot%20shape%20will,then%20the%20distribution%20is%20symmetric.\"Documentation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some transformation to see if you can get \n",
    "# box plots that indicate a normal distribution\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x ='Borough',y ='', data=complaints_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Median, Mode, and Random Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanImputation(data):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `data`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\"\n",
    "    column_means = data.mean()\n",
    "    result = data.fillna(column_means)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianImputation(data):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `data`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\" \n",
    "    column_medians = data.median()\n",
    "    return data.fillna(column_medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode imputation is typically for categorical variables\n",
    "def modeImputation(data):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `data`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\"\n",
    "    column_modes = data.mode()\n",
    "    return data.fillna(column_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randImputation(df):\n",
    "    \"\"\" \n",
    "    ---Description---\n",
    "    \n",
    "    Arguments:\n",
    "    `data`: \n",
    "    \n",
    "    Outputs:\n",
    "    `result`: \n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    for c in list(df.columns):\n",
    "        if df[c].dtype == '':\n",
    "            result[c] = result[c].fillna(value=np.random.randint\n",
    "            (complaints_df[c].quantile(0.34), complaints_df[c].quantile(0.68)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Dataframe.interpolate()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html \"Documentation\") Fill NaN values using an interpolation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘spline’, ‘barycentric’, ‘polynomial’\n",
    "# some of the parameters may need the argument \"order=\"\n",
    "continous_quality(complaints_df.interpolate(method='linear'))\n",
    "#complaints_df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bfill, ffill\n",
    "continous_quality(complaints_df.fillna(method='ffill'))\n",
    "#complaints_df.fillna(method='ffill').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try rapping complaints_df in the imputation and transformation functions to see how the graphs change\n",
    "complaints_df\\\n",
    ".hist(column=['asbestos_count',\n",
    "              'dust_count',\n",
    "              'gasses_count',\n",
    "              'mold_count',\n",
    "              'ventilation_count'],\n",
    "bins=10, figsize=(12, 8), alpha=0.6, grid=False, rwidth=0.8)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions\n",
    "\n",
    "[Dataframe.std()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html \"Documentation\") Return sample standard deviation over requested axis.\n",
    "\n",
    "[Dataframe.min()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.min.html \"Documentation\") Return the minimum of the values over the requested axis.\n",
    "\n",
    "[Dataframe.mean()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html \"Documentation\") Return the mean of the values over the requested axis.\n",
    "\n",
    "[Dataframe.div()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.div.html \"Documentation\") Get Floating division of dataframe and other, element-wise."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
